<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

        <!-- Bootstrap 4 -->
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
        <link type="text/css" rel="stylesheet" href="./main.css">

        <title>Test Page</title>
    </head>
    <body>
        <div class="container">
            <div class="row">
                <div class="col text-center">
                    <h1>Shift Invariance Experiments</h1>
                    <p>
                        Jake H. Lee, Columbia University
                        <br>
                        Dr. Junfeng Yang, Columbia University
                        <br>
                        Dr. Atlas Wang, Texas A&M University
                    </p>
                </div>
                
            </div>
            <div class="row">
                <div class="col-md-2"></div>
                <div class="col-md-8">
                    <h5 class="text-center">Description</h5>
                    <p>
                        Modern convolutional neural networks for object image classification have been shown to be very sensitive to the location of the object in the image. This may be due to biases in the training data or the architecture of the model itself. In this experiment, we evaluate such sensitivity by exhaustively shifting an object in an image and comparing features extracted from hidden layers and target class probabilities.
                    </p>
                    <h5 class="text-center">Datasets</h5>
                    <p>
                        We generated datasets by shifting a transparent patch across a background image, or by cropping a patch from a larger image. The currently available datasets are:
                        <ul>
                            <li>
                                <b>berry:</b> A 100x100 transparent image of a strawberry was shifted with stride 1 across a 224x224 completely white background. Its target class in ILSVRC2012 is <code>949: 'strawberry'</code>.
                            </li>
                            <li>
                                <b>plane:</b> A 224x224 image was cropped from a 356x356 image of a military jet with stride 1. Its target class in ILSVRC2012 is <code>895: 'warplane, military plane'</code>.
                            </li>
                        </ul>
                    </p>
                    <h5 class="text-center">Models</h5>
                    <p>
                        We evaluate pretrained models provided by torchvision <a href="https://pytorch.org/docs/stable/torchvision/models.html">[link]</a> and pretrained antialiased models provided with "Making Convolutional Networks Shift-Invariant Again" by Richard Zhang, published in ICML 2019 <a href="https://github.com/adobe/antialiased-cnns">[link]</a>. Details regarding training are:
                        <ul>
                            <li>
                                <b>torchvision: </b> Models were trained on ILSVRC2012. Data was augmented with random resized cropping <a href="https://github.com/pytorch/vision/blob/d2c763e14efe57e4bf3ebf916ec243ce8ce3315c/torchvision/transforms/transforms.py#L607">[docs]</a> and random horizontal flipping. The training script is available <a href="https://github.com/pytorch/vision/blob/master/references/classification/train.py">[here]</a>.
                            </li>
                            <li>
                                <b>antialiased: </b> Models were trained on ILSVRC2012. Architectures were directly modified from the torchvision models. Data was agumented with random resized cropping <a href="https://github.com/pytorch/vision/blob/d2c763e14efe57e4bf3ebf916ec243ce8ce3315c/torchvision/transforms/transforms.py#L607">[docs]</a> and random horizontal flipping. Training information is available <a href="https://github.com/adobe/antialiased-cnns/blob/master/README_IMAGENET.md">[here]</a>.
                            </li>
                        </ul>
                    </p>
                    <h5 class="text-center">Models and Layers</h5>
                    <p>
                        The following models and layers are currently available:
                        <ul>
                            <li>Alexnet
                                <ul>
                                    <li><b>fc6: </b> The first fully connected layer, before ReLU.</li>
                                    <li><b>fc6relu: </b> The fc6 layer after the ReLU activation function.</li>
                                    <li><b>fc7: </b> The penultimate fully connected layer, before ReLU.</li>
                                    <li><b>fc7relu: </b> The fc7 layer after the ReLU activation function.</li>
                                    <li><b>fc: </b> The final fully connected layer, before softmax.</li>
                                    <li><b>class: </b> Target class probability.</li>
                                </ul>
                            </li>
                            <li>Resnet50
                                <ul>
                                    <li><b>fc: </b> The final fully connected layer, before softmax.</li>
                                    <li><b>class: </b> Target class probability.</li>
                                </ul>
                            </li>
                        </ul>
                    </p>

                    <h5 class="text-center">Method</h5>
                    <p>
                        For each image in each dataset, we extract hidden layer activations (currently, only fully connected layers) as 1-dimensional feature vectors. Then, after selecting an anchor feature, we calculate the cosine similarity between it and all other features. These values are plotted as a heatmap in 2 dimensions (each axis representing the object shifting in each dimension). A fully shift-invariant network would have features that do not change depending on object location (a cosine similarity of 1 across the heatmap).
                        <br>
                        For each image in each dataset, we also retrieve the confidence of the corresponding target class. These values are not comparative, and are simply plotted on the heatmap. These values are represented as the "class" layer.
                    </p>
                    <h5 class="text-center">Results</h5>
                    <p>
                        The tool below allows for interactive browsing of the results. Different models, layers, and datasets can be selected with the dropdown menus. For features extracted from hidden layers, the feature to calculate the cosine similarity against (the "Anchor Image") can also be selected.
                        <br>
                        The two heatmaps for each model/layer/dataset/anchor combination is plotted below. The heatmap on the left shows results from the torchvision model, and the heatmap on the right shows results from the antialiased model. Sliders on the bottom can change the minimum value for the colormap (the maximum value is fixed at 1.0). Finally, hovering over the plot shows the corresponding image and value on the left panel.
                    </p>
                </div>
                <div class="col-md-2"></div>
            </div>
            <div class="row">
                <div class="col-md-4">
                    <!-- Select model and layer -->
                    <div class="form-group">
                        <label for="selectModel">Select Model:</label>
                        <select class="form-control" id="selectModel">
                            <option value="resnet50">Resnet50</option>
                            <option value="alexnet">Alexnet</option>
                        </select>
                        <br>
                        <label for="selectLayer">Select Layer:</label>
                        <select class="form-control" id="selectLayer">
                            <option value="fc">fc</option>
                            <option value="class">class</option>
                        </select>
                    </div>
                </div>
                <div class="col-md-4">
                    <!-- Select dataset and show description -->
                    <div class="form-group">
                        <label for="selectData"> Select Dataset:</label>
                        <select id="selectData" class="form-control">
                            <option value="berry">berry</option>
                            <option value="plane">plane</option>
                        </select>
                    </div>

                </div>
                <div class="col-md-4">
                    <!-- Select comparison anchor if applicable -->
                    <div class="form-group">
                        <label for="selectAnchor"> Select Anchor Image:</label>
                        <select class="form-control" id="selectAnchor">
                            <option value="tl">top left</option>
                            <option value="tr">top right</option>
                            <option value="c">center</option>
                            <option value="bl">bottom left</option>
                            <option value="br">bottom right</option>
                        </select>
                        <p>For patch-shifting datasets, "top left" indicates the position of the patch relative to the image. For cropping datasets, "top left" indicates which part of the image was cropped, not the object location.</p>
                    </div>
                </div>
            </div>
            <div class="row">
                <div class="col-md-2">
                    <!-- Show anchor img and current moused img -->
                    <div class="comp-div">
                        <img src="data/images/berry/tl.png" alt="anchor image" class="img-fluid back-image" id="anchorImage">
                    </div>
                    <span id="metric">cosine similarity = </span>
                    <br>
                    <span id="cos">1.0</span>
                    <div class="comp-div">
                        <img src="data/images/blank.png" alt="hover image" class="img-fluid back-image" id="hoverImage">
                        <img src="" alt="patch image" class="patch-image" id="patchImage">
                    </div>
                    <p>(simulated with css)</p>
                </div>
                <div class="col-md-5">
                    <!-- Pytorch pretrained model heatmap-->
                    <div><h4>Torchvision pretrained model</h4></div>
                    <div id="pytorchVis"></div>
                    <div class="form-group">
                        <label for="plot1range">colormap min: <span id="min1">0.965</span></label>
                        <input type="range" class="form-control-range" id="plot1range">
                    </div>
                </div>
                <div class="col-md-5">
                    <!-- Antialiased pretrained model heatmap-->
                    <div><h4>Bin-5 AA model (Zhang 2019, ICML)</h4></div>
                    <div id="pytorchAAVis"></div>
                    <div class="form-group">
                        <label for="plot2range">colormap min: <span id="min2">0.965</span></label>
                        <input type="range" class="form-control-range" id="plot2range">
                    </div>
                </div>
            </div>
        </div>
        
        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="plotting.js"></script>
    </body>
</html>